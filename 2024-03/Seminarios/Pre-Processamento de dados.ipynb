{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal componente analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Creamos datos de ejemplo (puedes usar los tuyos)\n",
    "# En este caso, generamos una matriz de datos con 3 variables y 100 observaciones\n",
    "np.random.seed(123)\n",
    "X = np.random.randn(100, 3)\n",
    "\n",
    "# Aplicar PCA\n",
    "pca = PCA(n_components=2, whiten=True)  # Queremos reducir a 2 componentes principales\n",
    "# whiten True var = 1\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Mostrar la cantidad de varianza explicada por cada componente\n",
    "print(\"Varianza explicada por cada componente:\", pca.explained_variance_ratio_)\n",
    "print(\"Valores propios de cada componente:\", pca.explained_variance_)\n",
    "# Graficar los datos en el espacio reducido de los componentes principales\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1])\n",
    "plt.title('PCA: Primeras dos componentes principales')\n",
    "plt.xlabel('Componente principal 1')\n",
    "plt.ylabel('Componente principal 2')\n",
    "plt.show()\n",
    "\n",
    "# print(\"Datos proyectados (nueva representación):\", X_pca) #scores\n",
    "\n",
    "print(\"Componentes principales (vectores de dirección):\", pca.components_)\n",
    "\n",
    "# Adicionar gráfico de barras para un autovector específico (primer componente principal)\n",
    "autovector = pca.components_[1]  # Seleccionamos el primer autovector\n",
    "\n",
    "# Crear gráfico de barras para mostrar los valores del primer componente principal\n",
    "plt.bar(range(len(autovector)), autovector, color='b', alpha=0.7)\n",
    "plt.xlabel('Índice de Variable Original')\n",
    "plt.ylabel('Valor del Autovector (Componente Principal)')\n",
    "plt.title('Valores del Primer Componente Principal (Autovector)')\n",
    "plt.xticks(range(len(autovector)), ['Var1', 'Var2', 'Var3'])  # Nombres opcionales de las variables originales\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factor Analisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "\n",
    "# Creamos datos de ejemplo\n",
    "# Aquí se genera una matriz con 100 observaciones y 5 variables\n",
    "np.random.seed(42)\n",
    "X = np.random.normal(size=(100, 5))\n",
    "\n",
    "# Aplicar Análisis Factorial para reducir la dimensionalidad a 2 factores\n",
    "fa = FactorAnalysis(n_components=2)\n",
    "X_fa = fa.fit_transform(X)\n",
    "\n",
    "# Graficar el resultado del análisis factorial\n",
    "plt.scatter(X_fa[:, 0], X_fa[:, 1])\n",
    "plt.xlabel('Factor 1')\n",
    "plt.ylabel('Factor 2')\n",
    "plt.title('Análisis Factorial (2 Factores)')\n",
    "plt.show()\n",
    "\n",
    "# Mostrar los factores obtenidos (cargas factoriales)\n",
    "print(\"Cargas factoriales (coeficientes de los factores):\\n\", fa.components_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Independent component analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import FastICA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f = 5  # frecuencia senoide\n",
    "fs = 50  # frecuencia de muestreo\n",
    "ta = 1/fs\n",
    "t = np.arange(0, 1, ta)  # vector de tiempo corregido\n",
    "\n",
    "# Señal senoide\n",
    "s1 = np.sin(2 * np.pi * f * t)\n",
    "\n",
    "# Ruido Gaussiano\n",
    "a = 0.5  # Define un valor para la amplitud del ruido\n",
    "s6 = np.random.randn(len(t)) * a\n",
    "\n",
    "# Señal compuesta\n",
    "s7 = s1 + s6\n",
    "\n",
    "# Concatenación de señales\n",
    "s = np.vstack((s1, s7))  # Mejor usar vstack para concatenar señales en diferentes filas\n",
    "\n",
    "# Transponer la matriz de señales\n",
    "S1 = np.transpose(s)\n",
    "\n",
    "# Matriz de mezcla\n",
    "M = np.array([[-1, 3], [2, 5]])\n",
    "\n",
    "# Mezcla de señales\n",
    "X = np.dot(S1, M.T)  # Multiplicación matricial con np.dot\n",
    "\n",
    "# Aplicar FastICA\n",
    "ica = FastICA(n_components=2)\n",
    "S = ica.fit_transform(X)  # Señales recuperadas por ICA\n",
    "\n",
    "# Graficar\n",
    "plt.subplot(3, 2, 1)\n",
    "plt.plot(t, X[:, 0]) \n",
    "plt.title('Mezcla 1')\n",
    "\n",
    "plt.subplot(3, 2, 2)\n",
    "plt.plot(t, X[:, 1]) \n",
    "plt.title('Mezcla 2')\n",
    "\n",
    "plt.subplot(3, 2, 3)\n",
    "plt.plot(t, S[:, 0])\n",
    "plt.title('Recuperado 1')\n",
    "\n",
    "plt.subplot(3, 2, 4)\n",
    "plt.plot(t, S[:, 1])\n",
    "plt.title('Recuperado 2')\n",
    "\n",
    "plt.subplot(3, 2, 5)\n",
    "plt.plot(t, S1[:, 0])\n",
    "plt.title('Original 1')\n",
    "plt.xlabel(\"Tiempo\")\n",
    "\n",
    "plt.subplot(3, 2, 6)\n",
    "plt.plot(t, S1[:, 1])\n",
    "plt.title('Original 2')\n",
    "plt.xlabel(\"Tiempo\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import DictionaryLearning\n",
    "\n",
    "# Generamos un banco de datos de ejemplo\n",
    "# Creamos una matriz de datos X con muestras aleatorias\n",
    "rng = np.random.RandomState(42)\n",
    "X = rng.randn(100, 64)  # 100 muestras con 64 características cada una\n",
    "\n",
    "# Aplicamos Sparse Coding (Dictionary Learning)\n",
    "dict_learner = DictionaryLearning(n_components=32, transform_algorithm='lasso_lars', transform_alpha=0.1, random_state=0)\n",
    "X_sparse = dict_learner.fit_transform(X)\n",
    "\n",
    "# Mostramos el diccionario aprendido y las representaciones sparse de los datos\n",
    "dictionary = dict_learner.components_  # Diccionario aprendido\n",
    "print(\"Diccionario aprendido (shape):\", dictionary.shape)\n",
    "print(\"Datos esparcidos (shape):\", X_sparse.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non negative matrix factorization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Crear datos de ejemplo no negativos\n",
    "# Generamos una matriz de datos positivos (por ejemplo, representando intensidad)\n",
    "X_nonneg = np.abs(np.random.normal(size=(100, 5)))\n",
    "\n",
    "# Aplicar NMF para descomponer la matriz en dos matrices de menor dimensión\n",
    "nmf = NMF(n_components=2, random_state=42)\n",
    "W = nmf.fit_transform(X_nonneg)  # Matriz de características latentes\n",
    "H = nmf.components_              # Matriz de pesos\n",
    "\n",
    "# Mostrar las matrices de la descomposición\n",
    "print(\"Matriz W (Combinación lineal de los componentes):\\n\", W)\n",
    "print(\"\\nMatriz H (Componentes latentes):\\n\", H)\n",
    "\n",
    "# Graficar la combinación lineal de los componentes\n",
    "plt.scatter(W[:, 0], W[:, 1])\n",
    "plt.xlabel('Componente Latente 1')\n",
    "plt.ylabel('Componente Latente 2')\n",
    "plt.title('Factorización No Negativa de Matrices (NMF)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Datos de ejemplo\n",
    "data = {\n",
    "    'age': [25, 30, 35, np.nan, 45],\n",
    "    'salary': [50000, 60000, np.nan, 80000, 100000],\n",
    "    'experience': [1, 3, np.nan, 7, 9]\n",
    "}\n",
    "\n",
    "# Crear el DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Inicializar el imputador KNN con 2 vecinos\n",
    "imputer = KNNImputer(n_neighbors=1)\n",
    "\n",
    "# Imputar los valores faltantes\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "\n",
    "# Convertir el resultado imputado de nuevo a DataFrame\n",
    "df_imputed = pd.DataFrame(df_imputed, columns=df.columns)\n",
    "\n",
    "# Mostrar el resultado\n",
    "print(df)\n",
    "print(df_imputed)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
